# cnn_ddp_torchrun.py

import argparse
import os
import torch
import torch.distributed as dist  # åˆ†å¸ƒå¼é€šä¿¡æ¨¡å— (Distributed)
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, DistributedSampler  # åˆ†å¸ƒå¼é‡‡æ ·å™¨
from torchvision import transforms, datasets
from torch.nn.parallel import DistributedDataParallel as DDP  # åˆ†å¸ƒå¼æ¨¡å‹åŒ…è£…å™¨
import matplotlib.pyplot as plt

from model import SoftmaxRegressionModel  
from utilis.tools import show_images  

def parse_args():
    parser = argparse.ArgumentParser(description='Distributed CNN training with torchrun')
    parser.add_argument('--lr', type=float, default=0.001)  
    parser.add_argument('--batch_size', type=int, default=128)  
    parser.add_argument('--epochs', type=int, default=50)  
    parser.add_argument('--save_path', type=str, default='cnn_model_ddp.pth')  # æ¨¡å‹ä¿å­˜è·¯å¾„
    return parser.parse_args()

transform = transforms.Compose([
    transforms.ToTensor(),  
    transforms.Normalize((0.5,), (0.5,)) 
])

def get_fashion_mnist_labels(labels):
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]


# ğŸ§© æ¨¡å—ï¼šåˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–ï¼Œè¿”å›å½“å‰è¿›ç¨‹çš„ GPU ç¼–å·
def setup_distributed():
    dist.init_process_group(backend='nccl')  # åˆå§‹åŒ–é»˜è®¤è¿›ç¨‹ç»„ï¼Œä½¿ç”¨ NCCL åç«¯ï¼ˆé«˜æ€§èƒ½ GPU é€šä¿¡ï¼‰
    local_rank = int(os.environ["LOCAL_RANK"])  # è·å–å½“å‰è¿›ç¨‹çš„ GPU ç¼–å·ï¼ˆtorchrun è‡ªåŠ¨è®¾ç½®ï¼‰
    torch.cuda.set_device(local_rank)  # è®¾ç½®è¯¥è¿›ç¨‹ä½¿ç”¨çš„ GPU
    return local_rank  

# ğŸ§© æ¨¡å—ï¼šé”€æ¯è¿›ç¨‹ç»„
def cleanup_distributed():
    dist.destroy_process_group()  # æ¸…é™¤æ‰€æœ‰è¿›ç¨‹é—´çš„é€šä¿¡èµ„æº


def train(args):
    local_rank = setup_distributed()  # è®¾ç½®å½“å‰è¿›ç¨‹ GPUã€åˆå§‹åŒ–é€šä¿¡ç»„
    device = torch.device(f"cuda:{local_rank}")  # å‘Šè¯‰ PyTorch å½“å‰è¿›ç¨‹ä½¿ç”¨å“ªä¸ª GPU
    train_dataset = datasets.FashionMNIST(root='/mnt/bn/occupancy3d/workspace/mzj/DL/09/data', train=True, download=True, transform=transform)
    sampler = DistributedSampler(train_dataset)  # æ¯ä¸ªè¿›ç¨‹å°†è¯»å–ä¸åŒçš„æ•°æ®ï¼ˆä¸é‡å¤ï¼‰
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler) # åˆ›å»º DataLoaderï¼Œä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨

    model = SoftmaxRegressionModel().to(device)
    model = DDP(model, device_ids=[local_rank])  # ä½¿ç”¨ DDP åŒ…è£…æ¨¡å‹ï¼Œå®ç°å¤šè¿›ç¨‹ä¹‹é—´çš„æ¢¯åº¦åŒæ­¥


    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    # åªåœ¨ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰ä¸Šæ˜¾ç¤ºè®­ç»ƒå›¾åƒ
    if dist.get_rank() == 0:  # rank æ˜¯å½“å‰è¿›ç¨‹ç¼–å·ï¼Œrank = 0 æ˜¯ä¸»è¿›ç¨‹
        X, y = next(iter(train_loader))
        X_show, y_show = X[:16], y[:16]
        fig, _ = show_images(X_show.reshape(16, 28, 28), 4, 4, titles=get_fashion_mnist_labels(y_show))
        fig.savefig("train_images.png")
        print("Train images saved to train_images.png")

    for epoch in range(args.epochs):
        model.train()
        sampler.set_epoch(epoch)  # æ¯è½®è®­ç»ƒéƒ½è¦è®¾ç½® epochï¼Œç¡®ä¿ä¸åŒ epoch çš„æ•°æ®åˆ’åˆ†ä¸åŒ
        running_loss = 0.0

        for features_batch, labels_batch in train_loader:
            features_batch = features_batch.to(device)
            labels_batch = labels_batch.to(device)

            preds = model(features_batch)  

            loss = criterion(preds, labels_batch)  
            optimizer.zero_grad()  
            loss.backward()        
            optimizer.step()       

            running_loss += loss.item()

        # ä»…ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—
        if dist.get_rank() == 0:
            avg_loss = running_loss / len(train_loader)
            print(f"[Epoch {epoch+1}] Loss: {avg_loss:.4f}")

    # ä»…ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹
    if dist.get_rank() == 0:
        torch.save(model.module.state_dict(), args.save_path)  # model.module æ˜¯ DDP åŒ…è£…çš„åŸå§‹æ¨¡å‹
        print(f"Model saved to {args.save_path}")

    cleanup_distributed()  # é‡Šæ”¾é€šä¿¡èµ„æº
    print("Distributed training completed.")

# ğŸ§© æ¨¡å—ï¼šæ¨ç†å‡½æ•°ï¼ˆéåˆ†å¸ƒå¼ï¼‰
def inference(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SoftmaxRegressionModel().to(device)
    model.load_state_dict(torch.load(args.save_path, map_location=device))  
    model.eval()

    test_dataset = datasets.FashionMNIST(root='/mnt/bn/occupancy3d/workspace/mzj/DL/09/data', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8)

    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

    test_data, test_target = next(iter(test_loader))
    test_data, test_target = test_data.to(device), test_target.to(device)

    with torch.no_grad():
        test_output = model(test_data)
        pred_labels = test_output.argmax(dim=1)

    # å¯è§†åŒ–å‰ 16 å¼ å›¾åƒçš„é¢„æµ‹ç»“æœ
    plt.figure(figsize=(8, 8))
    for i in range(16):
        plt.subplot(4, 4, i+1)
        plt.imshow(test_data[i].cpu().numpy().squeeze(), cmap='gray')
        plt.title(f"True: {classes[test_target[i]]}\nPred: {classes[pred_labels[i]]}", fontsize=8)
        plt.axis('off')
    plt.tight_layout()
    plt.savefig("inference_results.png")
    print("Inference results saved to inference_results.png")
    plt.show()


if __name__ == "__main__":
    args = parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°
    train(args)          # å¼€å§‹è®­ç»ƒï¼ˆtorchrun ä¼šè‡ªåŠ¨åœ¨æ¯å¼  GPU å¯åŠ¨ä¸€ä¸ªè¿›ç¨‹ï¼‰

    # ä»…åœ¨ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰ä¸Šæ‰§è¡Œæ¨ç†
    if int(os.environ.get("RANK", 0)) == 0: # dictionary.get(key, default_value)
        inference(args)